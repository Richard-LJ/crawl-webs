import requests
import re
from bs4 import BeautifulSoup
import os
import time

def getHtmlText(url):
    try:
        r = requests.get(url, timeout=30, headers=headers)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ''

def parseHtml(imgs, html):# 找出图片的链接
    soup = BeautifulSoup(html, "html.parser")
    ls = soup.find('div', class_='photo').find_all('img')
    for i in ls:
        imgs.append(i['src'])

def downloadImg(i, num, dir):
    response = requests.get(i, headers=headers)
    filename = dir + '/' + str(num) + i.split('/')[-1]
    with open(filename, 'wb') as f:  # 写入文件
        f.write(response.content)

def Findperson(html):
    r = re.compile('/bz/nxxz/[a-z]{4}/\d{6}.html')  # 找出一个人的页面
    urls = re.findall(r, html)
    urls = list(set(urls))  # 一个主页面的全部人
    return urls

def main():
    x = input('页数：')
    imgs = []

    dir = './妹纸图'  # 创建文件
    if not os.path.exists(dir):
        os.makedirs(dir)

    headers = {
        "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36",
        "cookie": "_uab_collina=157258226625083933457665; thw=cn; t=36d1cd24cf0143fb6accdf025534d197; enc=97GhrHhKkErSIlgzQuOf4gDv8yB1IDMrzS%2FqNp8OhQXosfA5%2Bpm6Vj4%2B%2FjCYIIsIglI%2FeakHaMTRg2bsOCGe%2Fg%3D%3D; hng=CN%7Czh-CN%7CCNY%7C156; cookie2=130c64bc2ccdc80f7d3858f7c4143707; _tb_token_=ede063be6e3ee; XSRF-TOKEN=a5e7e17c-f1d8-4ece-8f63-20dd0adc170c; mt=ci=0_0; cna=PpdBFupPaykCAbdAPqcqw59D; isg=BFlZdWsdY9ah6DznWNPShWvxaEwz5k2YGu7yC3sO5wD_gngUwTA4a72QgAZROuXQ; l=cBLufjhqqvUwqS6yBOCZhurza7799IRAguPzaNbMi_5CU6L65G7Oovk9xFp6cjWdOrYp4-ERe5p9-eteiNF7dhspXUJ1."
    }
    for i in range(1, int(x) + 1):
        url = 'http://www.jj20.com/bz/nxxz/list_7_'+str(i)+'.html'#各个主页
        html = getHtmlText(url)
        print('>>>'+ url)

        urls = Findperson(html)

        count = 1
        for i in urls:#进入每个人的页面
            #pattern = re.compile('[a-z]{4}/\d{6}')
            #a = (''.join(pattern.findall(i)))
            a = i

            for i in range(1, 15):#每一张的链接
                if i == 1:
                    url = 'http://www.jj20.com/' + str(a).split('.')[0] + '.html'
                else:
                    url = 'http://www.jj20.com/' + str(a).split('.')[0] + '_' + str(i) + '.html'
                print(url)

                try:
                    html = getHtmlText(url)
                    parseHtml(imgs, html)
                except:
                    continue

                num = 1
                for i in imgs:#下载每张图片
                    downloadImg(i, num, dir)
                    #time.sleep(1)
                    num += 1

main()





